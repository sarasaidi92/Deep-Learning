{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ed84205-e1a5-46a9-ab26-2722bd501ab3",
   "metadata": {},
   "source": [
    "**Deep Learnng:**\n",
    "The definition of Deep learning is that it is the branch of machine learning that is based on artificial neural network architecture. An artificial neural network or ANN uses layers of interconnected nodes called neurons that work together to process and learn from the input data.\n",
    "\n",
    "In a fully connected Deep neural network, there is an input layer and one or more hidden layers connected one after the other. Each neuron receives input from the previous layer neurons or the input layer. The output of one neuron becomes the input to other neurons in the next layer of the network, and this process continues until the final layer produces the output of the network. The layers of the neural network transform the input data through a series of nonlinear transformations, allowing the network to learn complex representations of the input data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2903bdfd-ed44-4feb-8bc4-d8835397ffb1",
   "metadata": {},
   "source": [
    "**Artificial neural networks:** Artificial neural networks are built on the principles of the structure and operation of human neurons. It is also known as neural networks or neural nets. An artificial neural network’s input layer, which is the first layer, receives input from external sources and passes it on to the hidden layer, which is the second layer. Each neuron in the hidden layer gets information from the neurons in the previous layer, computes the weighted total, and then transfers it to the neurons in the next layer. These connections are weighted, which means that the impacts of the inputs from the preceding layer are more or less optimized by giving each input a distinct weight. These weights are then adjusted during the training process to enhance the performance of the model.\n",
    "In a fully connected artificial neural network, there is an input layer and one or more hidden layers connected one after the other. Each neuron receives input from the previous layer neurons or the input layer. The output of one neuron becomes the input to other neurons in the next layer of the network, and this process continues until the final layer produces the output of the network. Then, after passing through one or more hidden layers, this data is transformed into valuable data for the output layer. Finally, the output layer provides an output in the form of an artificial neural network’s response to the data that comes in. \n",
    "\n",
    "Units are linked to one another from one layer to another in the bulk of neural networks. Each of these links has weights that control how much one unit influences another. The neural network learns more and more about the data as it moves from one unit to another, ultimately producing an output from the output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e28799d-d196-4042-8396-46b017be5803",
   "metadata": {},
   "source": [
    "**Difference between Machine Learning and Deep Learning:** machine learning and deep learning AI both are subsets of artificial intelligence but there are many similarities and differences between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309155dd-9dd9-43f3-a2ec-5bb627ff0316",
   "metadata": {},
   "source": [
    "**Machine Learning:**\n",
    "- Apply statistical algorithms to learn the hidden patterns and relationships in the dataset.\n",
    "- Can work on the smaller amount of dataset.\n",
    "- Takes less time to train the model.\n",
    "- Less complex and easy to interpret the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ecf4dd-3dbb-41e8-aec2-8e484b614b48",
   "metadata": {},
   "source": [
    "**Deep Learning:**\n",
    "- Uses artificial neural network architecture to learn the hidden patterns and relationships in the dataset.\n",
    "- Requires the larger volume of dataset compared to machine learning.\n",
    "- Takes more time to train the model.\n",
    "- More complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000ecf0f-2b7e-4d18-bec5-30498c6ae36d",
   "metadata": {},
   "source": [
    "**Types of neural networks:** \n",
    "Deep Learning models are able to automatically learn features from the data, which makes them well-suited for tasks such as image recognition, speech recognition, and natural language processing. The most widely used architectures in deep learning are feedforward neural networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs).\n",
    "\n",
    "1. Feedforward neural networks (FNNs) are the simplest type of ANN, with a linear flow of information through the network. FNNs have been widely used for tasks such as image classification, speech recognition, and natural language processing.\n",
    "2. Convolutional Neural Networks (CNNs) are specifically for image and video recognition tasks. CNNs are able to automatically learn features from the images, which makes them well-suited for tasks such as image classification, object detection, and image segmentation.\n",
    "3. Recurrent Neural Networks (RNNs) are a type of neural network that is able to process sequential data, such as time series and natural language. RNNs are able to maintain an internal state that captures information about the previous inputs, which makes them well-suited for tasks such as speech recognition, natural language processing, and language translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c8569f-8e1a-4271-9697-92013bbbf1b8",
   "metadata": {},
   "source": [
    "**Advantages of Deep Learning:**\n",
    "- High accuracy: Deep Learning algorithms can achieve state-of-the-art performance in various tasks, such as image recognition and natural language processing.\n",
    "- Automated feature engineering: Deep Learning algorithms can automatically discover and learn relevant features from data without the need for manual feature engineering.\n",
    "- Scalability: Deep Learning models can scale to handle large and complex datasets, and can learn from massive amounts of data.\n",
    "- Flexibility: Deep Learning models can be applied to a wide range of tasks and can handle various types of data, such as images, text, and speech.\n",
    "- Continual improvement: Deep Learning models can continually improve their performance as more data becomes available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a017862-25a2-4439-aab2-e812bb09d39f",
   "metadata": {},
   "source": [
    "**Disadvantages of Deep Learning:**\n",
    "- High computational requirements: Deep Learning AI models require large amounts of data and computational resources to train and optimize.\n",
    "- Requires large amounts of labeled data: Deep Learning models often require a large amount of labeled data for training, which can be expensive and time- consuming to acquire.\n",
    "- Interpretability: Deep Learning models can be challenging to interpret, making it difficult to understand how they make decisions.\n",
    "- Overfitting: Deep Learning models can sometimes overfit to the training data, resulting in poor performance on new and unseen data.\n",
    "- Black-box nature: Deep Learning models are often treated as black boxes, making it difficult to understand how they work and how they arrived at their predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ccd8b2-2e85-4807-b51c-30114519a4f6",
   "metadata": {},
   "source": [
    "In the process of building a neural network, one of the choices you get to make is what Activation Function to use in the hidden layer as well as at the output layer of the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0257ad7-dbd9-4539-a110-22ab5f21a737",
   "metadata": {},
   "source": [
    "**An Activation Function:** An activation function in the context of neural networks is a mathematical function applied to the output of a neuron. The purpose of an activation function is to introduce non-linearity into the model, allowing the network to learn and represent complex patterns in the data. Without non-linearity, a neural network would essentially behave like a linear regression model, regardless of the number of layers it has.\n",
    "\n",
    "The activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it. The purpose of the activation function is to introduce non-linearity into the output of a neuron.\n",
    "We know, the neural network has neurons that work in correspondence with weight, bias, and their respective activation function. In a neural network, we would update the weights and biases of the neurons on the basis of the error at the output. This process is known as back-propagation. Activation functions make the back-propagation possible since the gradients are supplied along with the error to update the weights and biases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64be299c-ae72-4d99-81f0-ba1c10bcc5e9",
   "metadata": {},
   "source": [
    "**Elements of a Neural Network:** \n",
    "- **Input Layer:** This layer accepts input features. It provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer. \n",
    "\n",
    "- **Hidden Layer:** Nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network. The hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer. \n",
    "\n",
    "- **Output Layer:** This layer bring up the information learned by the network to the outer world. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e03bb-ed48-4e82-8f48-1bed4527d9bc",
   "metadata": {},
   "source": [
    "**Variants of Activation Function:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ff6c87-3fc8-4e28-87d0-3b2fea9cd4cb",
   "metadata": {},
   "source": [
    " **1. _Linear Function_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7e3970-a28b-45aa-9a8d-5eeec118aeb6",
   "metadata": {},
   "source": [
    "- Equation : Linear function has the equation similar to as of a straight line i.e. y = x\n",
    "No matter how many layers we have, if all are linear in nature, the final activation function of last layer is nothing but just a linear function of the input of first layer.\n",
    "- Range : -inf to +inf\n",
    "- Uses : Linear activation function is used at just one place i.e. output layer.\n",
    "- Issues : If we will differentiate linear function to bring non-linearity, result will no more depend on input “x” and function will become constant, it won’t introduce any ground-breaking behavior to our algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0158db-6904-47d4-9664-44d7909f0978",
   "metadata": {},
   "source": [
    "**2. _Sigmoid Function_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec145e8f-9aaa-424d-8d52-6fd2a059919d",
   "metadata": {},
   "source": [
    "It is a function which is plotted as ‘S’ shaped graph.\n",
    "- Equation : A = 1/(1 + e-x)\n",
    "- Nature : Non-linear. Notice that X values lies between -2 to 2, Y values are very steep. This means, small changes in x would also bring about large changes in the value of Y.\n",
    "Value Range : 0 to 1\n",
    "- Uses : Usually used in output layer of a binary classification, where result is either 0 or 1, as value for sigmoid function lies between 0 and 1 only so, result can be predicted easily to be 1 if value is greater than 0.5 and 0 otherwise.\n",
    "- The major problem with using this activation function is that the output of this function saturates for larger values of x, as can be seen from the graph. Due to this the gradients value starts decreasing on every iteration of training and finally it becomes so small that weights remain almost the same. This problem is also known as the Vanishing Gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c0248-0250-4437-b2cc-9e26941e20c1",
   "metadata": {},
   "source": [
    "**3. _Tanh Function_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bcb6c6-2ccf-4f56-99d1-32c640639574",
   "metadata": {},
   "source": [
    "- The activation that works almost always better than sigmoid function is Tanh function also known as Tangent Hyperbolic function. It’s actually mathematically shifted version of the sigmoid function. Both are similar and can be derived from each other.\n",
    "- Equation :\n",
    "f(x) = tanh(x) = 2/(1 + e-2x) – 1\n",
    "OR\n",
    "tanh(x) = 2 * sigmoid(2x) – 1\n",
    "- Value Range : -1 to +1\n",
    "- Nature : non-linear\n",
    "- Uses : Usually used in hidden layers of a neural network as it’s values lies between -1 to 1 hence the mean for the hidden layer comes out be 0 or very close to it, hence helps in centering the data by bringing mean close to 0. This makes learning for the next layer much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df8fff9-866f-41b5-8ab2-066938af450f",
   "metadata": {},
   "source": [
    "**4. _RELU Function_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fa6b91-db0c-47d2-8fee-c47dadfe8c03",
   "metadata": {},
   "source": [
    "- It Stands for Rectified linear unit. It is the most widely used activation function. Chiefly implemented in hidden layers of Neural network.\n",
    "- Equation : A(x) = max(0,x). It gives an output x if x is positive and 0 otherwise.\n",
    "- Value Range : [0, inf)\n",
    "- Nature : non-linear, which means we can easily backpropagate the errors and have multiple layers of neurons being activated by the ReLU function.\n",
    "- Uses : ReLu is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations. At a time only a few neurons are activated making the network sparse making it efficient and easy for computation.\n",
    "In simple words, RELU learns much faster than sigmoid and Tanh function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d29408-f23d-450f-bd5d-1bb6aa098c21",
   "metadata": {},
   "source": [
    "**5. _Leaky RELU Function_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1567711d-5a44-48a0-aaf0-16172dd5c34e",
   "metadata": {},
   "source": [
    "- To mitigate the dying ReLU problem, Leaky ReLU introduces a small gradient for negative inputs, preserving some activity in the neurons.\n",
    "However, it struggles with consistency for negative inputs, using a fixed slope throughout training.\n",
    "- When implementing Leaky ReLU, it’s important to experiment with adjusting the learning rate, along with regular evaluations. This can help in determining the optimal configuration for Leaky ReLU in a given neural network.\n",
    "- In summary, Leaky ReLU is a valuable tool in the neural network toolkit, especially for addressing the limitations of the ReLU function.\n",
    "- Its ability to maintain a gradient flow through negative inputs makes it a popular choice for deep neural network models, though careful consideration is needed regarding the choice and handling of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523654f6-0224-47ba-869f-f3541a108746",
   "metadata": {},
   "source": [
    "**6. _ELU Function_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d5d3c-ee1b-408b-837e-d267e6b034c0",
   "metadata": {},
   "source": [
    "The Exponential Linear Unit (ELU) is an activation function for neural networks. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e2db4-3e43-4155-a25f-0cb48f056004",
   "metadata": {},
   "source": [
    "**7. _Softmax Function_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeccbee0-7250-456e-a3ad-48b0779d2f89",
   "metadata": {},
   "source": [
    "The Softmax function is also a type of sigmoid function but is handy when we are trying to handle multi- class classification problems.\n",
    "\n",
    "- Nature : non-linear\n",
    "- Uses : Usually used when trying to handle multiple classes. the Softmax function was commonly found in the output layer of image classification problems.The softmax function would squeeze the outputs for each class between 0 and 1 and would also divide by the sum of the outputs. \n",
    "- Output: The Softmax function is ideally used in the output layer of the classifier where we are actually trying to attain the probabilities to define the class of each input.\n",
    "- The basic rule of thumb is if you really don’t know what activation function to use, then simply use RELU as it is a general activation function in hidden layers and is used in most cases these days.\n",
    "- If your output is for binary classification then, sigmoid function is very natural choice for output layer.\n",
    "- If your output is for multi-class classification then, Softmax is very useful to predict the probabilities of each classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a64b80-39ca-4366-a4e2-33b2553e9034",
   "metadata": {},
   "source": [
    "**8. _Swish Function_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771a27b2-e66a-4585-8b09-04888937539d",
   "metadata": {},
   "source": [
    "The Swish activation function is a slight modification of the sigmoid function. The mathematical formula for this is:\n",
    "\n",
    "Swish(x) = x*sigmoid(beta{x})   , where beta is a scalable and trainable parameter.\n",
    "\n",
    "This function, unlike the ReLU function is not outputting 0 for all the negative values while it also maintains it’s characteristics for the positive values of x."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
